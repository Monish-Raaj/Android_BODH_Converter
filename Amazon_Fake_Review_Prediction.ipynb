{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Amazon_Fake_Review_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Monish-Raaj/Android_BODH_Converter/blob/master/Amazon_Fake_Review_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArAO_L5IeyaM"
      },
      "source": [
        "import csv, re\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "from random import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "start_time = time.time()\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1fUgP0MgFj0",
        "outputId": "ab52cb1b-d106-4dc3-ee16-e9d327ef9dd4"
      },
      "source": [
        "pip install pyphen"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (0.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNIqJKPTe7Z6"
      },
      "source": [
        "# We import some of the parsing and loading methods we had from previous tasks\n",
        "def parse_label(label):\n",
        "    if label == '__label2__':\n",
        "        return 'real'\n",
        "    else:\n",
        "        return 'fake'    \n",
        "\n",
        "def parse_verification(label):\n",
        "    if label == 'N':\n",
        "        return 0\n",
        "    else:\n",
        "        return 1      \n",
        "\n",
        "def flatten(lst):\n",
        "    for el in lst:\n",
        "        if isinstance(el, list):\n",
        "            yield from el\n",
        "        else:\n",
        "            yield el    \n",
        "\n",
        "# Convert line from input file into an id/text/label tuple\n",
        "def parseReview(reviewLine):\n",
        "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
        "    return (reviewLine[0], \\\n",
        "            reviewLine[8] + ' ' + reviewLine[7], \\\n",
        "            parse_label(reviewLine[1]), \\\n",
        "            reviewLine[2], \\\n",
        "            parse_verification(reviewLine[3]))\n",
        "\n",
        "# load data from a file and append it to the rawData\n",
        "def loadData(path, Text=None):\n",
        "    with open(path) as f:\n",
        "        reader = csv.reader(f, delimiter='\\t')\n",
        "        next(reader, None)  # skip the headers        \n",
        "        for line in reader:\n",
        "            (Id, Text, Label, Rating, Verified) = parseReview(line)\n",
        "            rawData.append((Id, Text, Label, Rating, Verified))\n",
        "            \n",
        "#Loading our set\n",
        "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
        "reviewPath = 'amazon_reviews.txt'\n",
        "# loadData(reviewPath)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW096vV9g2ZT",
        "outputId": "5dd9d99c-a687-47d3-fd5f-3b8227015a5c"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLg_DdPugB_f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "2870c172-fd77-4307-861d-f57d58f7e7cb"
      },
      "source": [
        "#We'll want to put it all in a dataframe, and add some additional features\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import pyphen\n",
        "dic = pyphen.Pyphen(lang='en')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def averageLen(lst):\n",
        "    lengths = [len(i) for i in lst]\n",
        "    return 0 if len(lengths) == 0 else (float(sum(lengths)) / len(lengths)) \n",
        "\n",
        "\n",
        "names = ['doc_id', 'label', 'rating', 'verified_purchase', \\\n",
        "         'pr_category', 'pr_id', 'pr_title', 'review_title', 'review_text']\n",
        "data = pd.read_csv('https://github.com/lievcin/amazon_deception/blob/master/amazon_reviews.txt', skiprows=[0], names=names, sep='\\t')\n",
        "data['verified_purchase'] = data['verified_purchase'].apply(lambda x: 1 if x=='Y' else 0)\n",
        "data['label'] = data['label'].apply(lambda x: 'real' if x=='__label2__' else 'fake')\n",
        "data['review_text_length'] = data.apply(lambda row: len(row.review_text), axis=1)\n",
        "data['review_title_length'] = data.apply(lambda row: len(row.review_title), axis=1)\n",
        "data['tokenized_review'] = data.apply(lambda row: re.sub(r\"(\\w)([.,;:!-?'\\\"”\\)])\", r\"\\1 \\2\", row['review_text']), axis=1)\n",
        "data['tokenized_review'] = data.apply(lambda row: re.sub(r\"([.,;:!-?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", row['tokenized_review']), axis=1)\n",
        "data['tokenized_review'] = data.apply(lambda row: re.sub(r\"<[^>]*>\", \"\", row['tokenized_review']), axis=1)\n",
        "data['tokenized_review'] = data.apply(lambda row: nltk.word_tokenize(row['tokenized_review'].lower()), axis=1)\n",
        "data['tokenized_review_string'] = data['tokenized_review'].apply(' '.join)\n",
        "data['tokens'] = data.apply(lambda row: [t for t in row['tokenized_review'] if t.isalpha()], axis=1)\n",
        "data['tokens'] = data.apply(lambda row: [t for t in row['tokens'] if t not in stop_words], axis=1)\n",
        "data['stopwords'] = data.apply(lambda row: [t for t in row['tokenized_review'] if t.isalpha()], axis=1)\n",
        "data['stopwords'] = data.apply(lambda row: [t for t in row['tokenized_review'] if t in stopwords.words('english')], axis=1)\n",
        "data['num_tokens'] = data.apply(lambda row: len(row['tokens']), axis=1)\n",
        "data['avg_len_tokens'] = data.apply(lambda row: averageLen(row['tokens']), axis=1)\n",
        "data['num_stopwords'] = data.apply(lambda row: len(row['stopwords']), axis=1)\n",
        "data['word_count'] = data.apply(lambda row: len(row['tokenized_review']), axis=1)\n",
        "data['sent_count'] = data.apply(lambda row: len(sent_tokenize(row['review_text'])), axis=1)\n",
        "data['syll_count'] = data.apply(lambda row: len(list(flatten([dic.inserted(text).split('-') for text in row['tokenized_review']]))), axis=1)\n",
        "data['flesch_kincaid'] = data.apply(lambda row: 206.835 - 1.015*row['word_count']/row['sent_count'] - 84.6*row['syll_count']/row['word_count'], axis=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ccccd23bd54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verified_purchase'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verified_purchase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Y'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'real'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__label2__'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'fake'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_text_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_title_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"(\\w)([.,;:!-?'\\\"”\\)])\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"\\1 \\2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7550\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7551\u001b[0m         )\n\u001b[0;32m-> 7552\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-ccccd23bd54e>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verified_purchase'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'verified_purchase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Y'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'real'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__label2__'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'fake'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_text_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_title_length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreview_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"(\\w)([.,;:!-?'\\\"”\\)])\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mr\"\\1 \\2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63zMNACkf81T"
      },
      "source": [
        "#Finally on to our classifier, the first thing is to add a column that will serve to capture the sentiment\n",
        "def get_sent(rating):\n",
        "    if rating < 3:\n",
        "        sent = 'negative'\n",
        "    elif rating > 3:\n",
        "        sent = 'positive'\n",
        "    else:\n",
        "        sent = 'mweh'\n",
        "    return sent\n",
        "\n",
        "data['sentiment'] = data.apply(lambda row: get_sent(row['rating']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsYGANA3jn11"
      },
      "source": [
        "#Now let's make some vectors from our data, these will be used by the classifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['review_text'], data['sentiment'], test_size=0.33, random_state=1)\n",
        "\n",
        "# Initialize a CountVectorizer and Tfidf objects\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
        "\n",
        "# Transform the training data using only the 'text' column values: count_train \n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using only the 'review_text' column values: count_test \n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU0lWt99jq92"
      },
      "source": [
        "# Import the necessary modules for our classifier\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
        "print('Confusion matrix:')\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVucfqOtjtiJ"
      },
      "source": [
        "# What if we used to Tfidf vectors instead?\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(tfidf_train, y_train)\n",
        "pred = nb_classifier.predict(tfidf_test)\n",
        "\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
        "print('Confusion matrix:')\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE9QjiSqjzi2"
      },
      "source": [
        "# Create the list of alphas: alphas\n",
        "import numpy as np\n",
        "alphas = np.arange(0, 1.5, .1)\n",
        "\n",
        "def train_and_predict(alpha):\n",
        "    nb_classifier = MultinomialNB(alpha=alpha)\n",
        "    nb_classifier.fit(tfidf_train, y_train)\n",
        "    pred = nb_classifier.predict(tfidf_test)\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "for alpha in alphas:\n",
        "    print('Alpha: ' + str(alpha) + ' score: ' + str(round(train_and_predict(alpha), 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeS3T2d5j2wD"
      },
      "source": [
        "alphas = np.arange(0, 1.5, .1)\n",
        "\n",
        "def train_and_predict(alpha):\n",
        "    nb_classifier = MultinomialNB(alpha=alpha)\n",
        "    nb_classifier.fit(count_train, y_train)\n",
        "    pred = nb_classifier.predict(count_test)\n",
        "    score = metrics.accuracy_score(y_test, pred)\n",
        "    return score\n",
        "\n",
        "for alpha in alphas:\n",
        "    print('Alpha: ' + str(alpha) + ' score: ' + str(round(train_and_predict(alpha), 3)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrPKn91YdD9_"
      },
      "source": [
        "\n",
        "#But we can also play with vectorizing ngrams instead of just unigrams\n",
        "print('Unigrams only:')\n",
        "count_vectorizer = CountVectorizer(stop_words='english')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "nb_classifier = MultinomialNB(alpha=0.5)\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "pred = nb_classifier.predict(count_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
        "print('Confusion matrix:')\n",
        "print(cm)\n",
        "\n",
        "print('Ngrams:')\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1, 7), analyzer='char')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "nb_classifier = MultinomialNB(alpha=0.2)\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "pred = nb_classifier.predict(count_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
        "print('Confusion matrix:')\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x16lWIEeexYK"
      },
      "source": [
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1, 7), analyzer='char')\n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "count_test = count_vectorizer.transform(X_test)\n",
        "svm_classifier = LinearSVC()\n",
        "# nb_classifier = MultinomialNB(alpha=0.2)\n",
        "svm_classifier.fit(count_train, y_train)\n",
        "# nb_classifier.fit(count_train, y_train)\n",
        "# pred = nb_classifier.predict(count_test)\n",
        "pred = svm_classifier.predict(count_test)\n",
        "score = metrics.accuracy_score(y_test, pred)\n",
        "print('Classifier accuracy: ' + str(round(100*score, 2)) + '%')\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=['negative', 'mweh', 'positive'])\n",
        "print('Confusion matrix:')\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PogUt9mRj9Qf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAXaPbXEqtvz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ_zgG7Uq4aC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}